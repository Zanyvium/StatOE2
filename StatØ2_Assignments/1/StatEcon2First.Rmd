---
title: "Stat Econ 2 First"
author: "Victor Z. Nygaard"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{verbatim}
   - \usepackage[language]{babel}
   - \usepackage[encoding]{inputenc}
   - \usepackage{hyperref}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \usepackage{amssymb}
   - \usepackage{mathtools}
   - \usepackage{nicefrac}
   - \usepackage{fullpage}
   - \usepackage{stmaryrd}
   - \usepackage{aligned-overset}
   - \usepackage{pdfpages}
date: "Last compiled on `r format(Sys.time(), '%d. %B, %Y')`"
# Hvordan får  vi den til at printe February i stedet for februar??????
output: html_document
---

------------------------------------------------------------------------

```{=html}
<!-- RMD tips:
1. CTRL+SHIFT+C RMD-comments-out the selected lines, with in a standard HTML comment-out format. 

2. CTRL+ALT+I inserts a new r codechunck

3. Pressing CTRL+SHIFT+ENTER when over a code chunck gives you a preview of the results of the chunck

4. CTRL+SHIFT+K gives you a preview of the entire resulting HTML file.

5. Note the different results of '#HS 1' and 
'# HS 1' (without the '') in the output

6. It is possible to compile regular R-scripts into Rmd files - this is done by pressing CTRL+SHIFT+K while attending any R-script. <<<- Though this apparently doesn't work for HS Problems.R for some reason!?!?!? ->>>

7. Selective use of the echo=c(...) option within code chuncks allows assignment of a variable, to show the assignment in the knitted document, and showing the value of the assignment seamlessly as well - see HS2.3

8. It is possible to include results of R-analysis such as summary statistics in LaTeX-equations in RMD, see HS2.3

9. Adding fig.align="center" to a code chunk centers any figures generated by the chunck.

9.1 note that properties of codechunks seem casesensitive; fig.align="center" centers a figure, but fig.align="Center" (with capital C) doesn't

10. A new subtitle needs a blank line before itself: 
'works:

#### HS 7
'

'doesn't:
blablabla
#### HS 7
'

'doesn't either:
<!-- blablabla ->
#### HS 7
'
11. Pressing F7 when marking, or hovering over a word will spellcheck the word

12. CTRL + - (minus) zooms out, CTRL + + (plus) zooms in

13. CTRL + D Deletes the current line, or current selection of lines

14. THE FOLLOWING SOURCE EDITOR FOLDING METHODS:
14.1 Collapse current fold: ALT + L
14.1.1: Expand current fold: SHIFT + ALT + L
14.2 Collapse "all" subfolds: ALT + O <- !?!! Note that this leaves a small letter 'o' in the text !!?!
14.2.1 Expand "all" subfolds: SHIFT + ALT + O
14.3 Collapse all other folds: ALT + 0 (zero)

15. SHIFT + ALT + J allows you to jump to specific parts of the document

16. Writing a new line with '...' will cause all previous output to be hidden in the knittet document

17. Writing (q<-5) around R code, will both assign and print the code upon assignment 

18. Note that 'attach' only has the scope of the current R-chunck.

19. One way to get pdf printout is to compile a html-printout, and then, in-browser, 'print' the HTML page as a pdf.

20. CTRL + SHIFT + M gives the pipe operator.

21. Pressing CTRL + F3 searches on the selected word.

22. CTRL
-->
```
<!-- ---?--- How do I create a closeable Rmd section, such that I do not have to scroll through the LaTeX commands each time? - !!! Can be done with '-----' through this also creates a line in the knittet document. -->

<!-- How do I publish and share the HTML as a viewable (and linkable) website - this can be done through github? -->

<!-- How can I share R markdown files such that multiple people can edit them at the same time? -->

<!-- Do we need parindent controls as in LaTeX? -->

<!-- Use of the cache function to reduce recompile times -->

<!-- How to close current subsection with a keyboard shortcut? How to close subsubsections,...? - !!!See RMD tip 14!!! -->

<!-- Chunk naming? -->

<!-- How to define variables such that they have scope within their own ## segment? -->

<!-- How do I delete all non-needed variables for each new section in R??? -->

<!-- LaTeX commands -->

\newcommand{\C}{\mathbb{C}} <!--- Komplekse tal --->

\newcommand{\R}{\mathbb{R}} <!--- Reelle tal--->

\newcommand{\Q}{\mathbb{Q}}

<!---Rationelle tal--->

\newcommand{\Z}{\mathbb{Z}}

<!---Hele tal--->

\newcommand{\N}{\mathbb{N}}

<!---Naturlige tal--->

\newcommand{\E}{\mathbb{E}}

<!---mean--->

\newcommand{\F}{\mathbb{F}}

<!---Baggrundsrum sigma-alg--->

\newcommand{\B}{\mathbb{B}}

<!---Borel sigma--->

\newcommand{\K}{\mathbb{K}}

<!---Generel field--->

\newcommand{\RB}{\overline{\R}}

<!---Udvidede reelle tal--->


\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\BR}{\mathcal{B}\left(\R\right)} <!---Borel på Reelle tal -->
\newcommand{\BRB}{\mathcal{B}\left(\RB\right))} <!---Borel på udvidede reelle tal -->


\newcommand{\mf}[1]{\mathfrak{#1}} 
\newcommand{\mcG}[2]{\mathcal{#1}^1(#2)} 
\newcommand{\mcGG}[4]{\mathcal{#1}_{#3}^{#2}(#4)}
\newcommand{\GMR}{\left(X,\ms{A},\mu\right)} <!---Generelt målrum -->

\newcommand{\PBS}{\lrp{\Omega,\F, P}}

<!--- Probability background space -->

\newcommand{\RMR}{\left(\R,\BR, \lambda\right)}

<!---Reelt målrum, m. Borel, og lebesgue mål. -->

\newcommand{\MRBPBR}{\mc{M}_{\RB}^+\left(\BR\right)}

<!---Mængden af positive målelige funktioner som sender (R,BR) over i (RB, BRB) -->

\newcommand{\MRPBR}{\mc{M}_{\R}^+\left(\BR\right)}

<!---Mængden af positive målelige funktioner som sender (R,BR) over i (R, BR) -->

<!---L_p spaces on [0,1] with m -->


\newcommand{\Lp}[1]{L_{#1}\lrp{\lrs{0,1},m}} 
\newcommand{\mclxy}{\mc{L}\lrp{X,Y}}

<!---Bounded linear functionals from X to Y -->

\newcommand{\mckxy}{\mc{K}\lrp{X,Y}}

<!---Compact Bounded linear functionals from X to Y -->

\newcommand{\mssr}{\ms{S}(\R)}

<!---The Schwartz space on $\R$ -->

<!---Arrows -->

\newcommand{\ra}{\rightarrow} <!---Konvergens pil højre -->
\newcommand{\nra}{\nrightarrow} <!---ikke Konvergens pil højre -->
\newcommand{\la}{\leftarrow} <!---Konv pil venstre -->
\newcommand{\nla}{\nleftarrow} <!---ikke Konvergens pil venstre -->
\newcommand{\lra}{\leftrightarrow} <!---højre venstre pil -->
\newcommand{\nlra}{\nleftrightarrow} <!---ikke højre venstre pil -->
\newcommand{\hra}{\hookrightarrow} <!---Injektiv  pil højre -->
\newcommand{\Ra}{\Rightarrow} <!---Implikations pil højre -->
\newcommand{\Lra}{\Leftrightarrow} <!---Bi-implikations pil -->
\newcommand{\Uda}{\Updownarrow} <!---Bi-implikations pil (op og ned) -->
\newcommand{\Da}{\Downarrow} <!---implikations pil (ned) -->
\newcommand{\rhpu}{\rightharpoonup} <!---Weak convergence in Hilbert spaces -->
\newcommand{\raas}{\overset{\textit{a.s.}}{\ra}} <!---Konvergens a.s. pil højre -->
\newcommand{\rap}{\overset{\textit{P}}{\ra}} <!---Konvergens P pil højre -->
\newcommand{\rad}{\overset{\textit{d}}{\ra}} <!---Konvergens d. pil højre -->
\newcommand{\asympt}{\overset{\textit{as}}{\sim}} <!--- asymptotisk konvergens -->


<!-- Equalities -->
\newcommand{\eqtx}[1]{\overset{\text{#1}}{=}} <!-- Insert text above equality -->
\newcommand{\eqas}{\eqtx{a.s.}} <!-- a.s. equality -->
\newcommand{\inse}{\overset{\cdot}{=}} <!-- Inserting values for parameters -->
\newcommand{\eqd}{\overset{d.}{=}} <!-- Equality of distribution -->

<!-- LHS & RHS calculations -->

\newcommand{\swel}{\overset{\swarrow}{=}} <!---Continue calculation on left hand side with equality -->
\newcommand{\sweq}{\overset{\swarrow}{\equiv}} <!---Continue calculation on left hand side with equivalence -->
\newcommand{\seel}{\overset{\searrow}{=}} <!---Continue calculation on right hand side with equality -->
\newcommand{\seeq}{\overset{\searrow}{\equiv}} <!---Continue calculation on right hand side with equivalence -->
\newcommand{\inse}{\overset{\cdot}{=}} <!--- Insert values in calculation -->

\newcommand{\PMX}{\mc{P}\left(X\right)} <!---Potensmængde af X -->
\newcommand{\comp}{\mathsf{c}} <!---Set compliment -->
\newcommand{\sm}{\setminus} <!---mængdedifferens -->

<!--- Parenteser --->
\newcommand{\lrp}[1]{\mathopen{}\left({#1}\right)\mathclose{}} <!-- \left("STUFF"\right) -->
\newcommand{\lrc}[1]{\mathopen{}\left\{{#1}\right\}\mathclose{}} <!-- \left\{"STUFF"\right\} -->
\newcommand{\lrs}[1]{\mathopen{}\left[{#1}\right]\mathclose{}} <!-- \left["STUFF"\right] -->
\newcommand{\lrb}[1]{\mathopen{}\left|{#1}\right|\mathclose{}} <!-- \left|"STUFF"\right| -->
\newcommand{\inner}[2]{\mathopen{}\left\langle #1, #2 \right\rangle\mathclose{}}  <!-- <\left"STUFF1","STUFF2"\right> -->
\newcommand{\norm}[1]{\mathopen{}\left\lVert#1\right\rVert\mathclose{}} <!-- \left||"STUFF"\right|| -->
\newcommand{\floor}[1]{\lfloor #1 \rfloor} <!---Floor function --->
\newcommand{\ceil}[1]{\lceil #1 \rceil} <!---ceil --->
\newcommand{\FFou}[1]{\mc{F}(#1)} <!---Fourier Transform notation 1 --->
\newcommand{\Fou}[1]{\widehat{#1}} <!---Fourier Transform notation 2 --->

<!--- Farver --->
\newcommand{\blue}[1]{\textcolor{blue}{{#1}}} <!--- Turning text blue --->
\newcommand{\red}[1]{\textcolor{red}{{#1}}} <!--- Turning text red --->
\newcommand{\green}[1]{\textcolor{green}{{#1}}} <!--- Turning text green --->
\newcommand{\purple}[1]{\textcolor{purple}{{#1}}} <!--- Turning text purple --->
\newcommand{\cyan}[1]{\textcolor{cyan}{{#1}}} <!--- Turning text cyan --->
\newcommand{\orange}[1]{\textcolor{orange}{{#1}}} <!--- Turning text orange --->

<!--- Oversetting bold accents --->
\newcommand{\boldhat}[1]{\mathbf{\hat{\text{$#1$}}}}
\newcommand{\boldbar}[1]{\mathbf{\bar{\text{$#1$}}}}
\newcommand{\boldtilde}[1]{\mathbf{\tilde{\text{$#1$}}}}
\newcommand{\boldcheck}[1]{\mathbf{\check{\text{$#1$}}}}

\newcommand{\indep}{\perp \!\!\! \perp} <!---independence --->
\newcommand{\colvec}[1]{\begin{pmatrix}{#1}\end{pmatrix}} <!-- Begin column vector - Doesn't seem to work with non-column vectors...-->


\newcommand{\nd}[2]{\mc{N}\lrp{{#1},{#2}}} <!-- Normal distribution -->
\newcommand{\dnd}[2]{\sim\mc{N}\lrp{{#1},{#2}}} <!-- Distributed as Normal distribution -->

<!-- \newcommand{\Rlogo}{![](R_logo.png){#id .class width=auto height=16px} } <!-- R logo implemented in text -->

<!-- Image insertion alla LaTeX doesn't seem to work too well..., but inserting the above gives the desired effect. -->

<!--???? \declareMathOperator{\SE}{SE} DOESN'T REALLY SEEM TO WORK????-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE) #semi-dangerous to set cache true globally :||
knitr::opts_chunk$set(fig.align = 'center')
library(tidyverse)
library(reshape2)
library(gridExtra)
library(xtable)
library(splines)
library(survival)
library(grid)
library(lubridate)
library(xts)
theme_set(theme_minimal())
library(MASS)
set.seed(314)
varnametotext <- function(v){
   deparse(substitute(v))
}
Stdresplot <- function(model, main = paste("(Estimate, Std. Res.)-plot of", deparse(substitute(model))), ylab ="Standardized residuals", ...) {
 fit <- fitted(model)
 rst <- rstandard(model)
 qplot(fit, rst, main = main, ylab = ylab, ylim = c(-max(3.2,max(abs(rst))), max(3.2,max(abs(rst)))) )+geom_hline(yintercept = 0) #Largest symmetric interval (around 0) of (-3.2,3.2) or (-largest absolute rst, largest absolute rst)
}
QQplotdraw <- function(model, main = paste("Normal QQ-plot of", deparse(substitute(model))), xlab = "Theoretical Quantiles", ylab ="Sample Quantiles", ...) {
   rst <- rstandard(model)
   #dataname <- getCall(lm_LT)$data
   ggplot(data = eval(getCall(model)$data), main = main, xlab = xlab, ylab = ylab) + geom_qq() + geom_qq_line() + aes(sample = rst)
} #main, xlab, ylab call do not work for some reason
StdresQQPlot <- function(model,...) {
   p1 <- Stdresplot(model,...)
   p2 <- QQplotdraw(model,...)
   #library(gridExtra)
   grid.arrange(p1,p2, ncol = 2)
}
```

# Stat Econ 2 Assignment 1
  
## Exercise 1


We might note that we are dealing with an AR(1) process with slope $0.9$ and, assuming a central requirement, central $t$ - distributed noise with ten degrees of freedom. We simulate noise terms $Z_t$ for $t=1,\ldots,1000$ 
```{r}
set.seed(314)
desiredlag <- 20
phi <- 0.9
n <- 10^3
Z <- rt(df = 10, n=n)
```
As such we may simulate the AR(1) process using the update scheme defining the 'stopped' AR(1) process
```{r, fig.align='center'}
X <- rep(NA,n)
X[1] <- Z[1]
for (j in 2:n) {
   X[j] <- phi*X[j-1]+Z[j]
}
ts.plot(X)
```

### a)
We plot the `acf` confidence bands:
```{r, fig.align = 'center'}
acf(X, lag.max = desiredlag, plot = T)
#acf(X, lag.max = desiredlag, plot = F)
```

<!-- We would also have done the permutations using a homemade function that would repeatedly use the sample function - I did not get this to work. -->
Assuming the requirement to sample from $X,$ and not from $Z$ which would be more in line with the theoretical foundation for permutations of data, which are in and of themselves understood here, to be a random reordering of all data points. We may create a matrix containing $10^3$ rows, each a permutation of the $X$ data as such:
```{r}
m <- 10^3
M <- matrix(NA, nrow = m, ncol = n)
for (i in 1:m) {
   M[i,] <- sample(X)
}
```
For each of these 'permuted' data sets, we will use `acf` to calculate sample autocorrelations for lags once again up to `desiredlag  =`$20,$ and then the $95\%$ quantiles for each of the lags:
```{r}
AutM <- apply(M,1,acf,lag.max=desiredlag, plot = F)
temp <- matrix(NA,nrow = m, ncol = desiredlag+1)
for (i in 1:1000) {
   temp[i,] <- AutM[[i]][[1]]
}
quanties <- apply(temp,2,quantile, prob=c(0.025,0.975))
```

We may then also plot the resulting quantiles:
```{r}
#ggplot(quanties) + geom_line(aes())
#groupby?
```


<!-- M<-matrix(NA,nrow=n,ncol=n) -->
<!-- # Ma<-matrix(NA, nrow = n, ncol = desiredlag) -->
<!-- # for (i in 1:n) { -->
<!-- #    M[,i] <- sample(X) -->
<!-- # } -->
<!-- #Ma <- apply(M,1,acf,lag.max = desiredlag, plot=F) -->


<!-- We should probabily do the permutations for the noise as these are iid!! -->
<!-- What does lag-wise mean? -->
<!-- What constitutes a permutation? -->
<!-- What are 1000 permutations of data - do we require 1000 confidence intervals? -->

<!-- ```{r} -->
<!-- acf(X, lag.max = 20, plot = F) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- XP <- sample(X,1000) -->
<!-- ts.plot(XP) -->
<!-- acf(XP, lag.max = 20, plot = T) -->
<!-- acf(X, lag.max = 20, plot = T) -->
<!-- ``` -->


### b)
We note that our AR(1) model is causal as $\lrb{\phi}<1.$ Following example 4.25, we will need to calculate the sample autocovariance function:
\begin{equation*}
\gamma_{n,X}\lrp{h}:=\frac{1}{n}\sum_{t=1}^{n-h}{\lrp{X_t-\overline{X}_n}\lrp{X_{t+h}-\overline{X}_n}}
\end{equation*}
and autocorrelation function:
\begin{equation*}
\rho_{n,X}\lrp{h}:=\frac{\gamma_{n,X}\lrp{h}}{\gamma_{n,X}\lrp{0}}
\end{equation*}
<!-- Note in this regard that calculated in ![](R_logo.png){#id .class width=auto height=16px}: $\overline{X}_n=$`mean(X)=`r mean(X)``. -->
Note thus in particular that we may calculate $\gamma_{n,X}\lrp{1},\,\gamma_{n,X}\lrp{0}$ in ![](R_logo.png){#id .class width=auto height=16px} with the following homemade function
```{r}
gammaf <- function(X,h) {
   n <- length(X)
   gamt <- 0
   for (t in 1:(n-h)) {
      tempt <- (X[t]-mean(X))*(X[t+h]-mean(X)) 
      gamt <- gamt + tempt
   }
   1/n*gamt
}
```

Yielding
```{r}
gammaf(X,1)
gammaf(X,0)
```
Such that for

$$
\hat{\phi}_n=\frac{\gamma_{n,X}\lrp{1}}{\gamma_{n,X}\lrp{0}}\equiv\rho_{n,X}\lrp{1}
$$
$$
\hat{\sigma}_n^2=\gamma_{n,X}\lrp{0}\lrp{1-\rho_{n,X}^2\lrp{1}}
$$
```{r}
(rho1 <- gammaf(X,1)/gammaf(X,0))
(phih <- rho1)
(sigmah2 <- gammaf(X,0)*(1-rho1^2))
```

Let $\nu=10$ be the degrees of freedom of our student-t distributed random noise $Z_t.$ As is surmised on page 47-48 in the lecture notes, in dealing with a causal AR(1) process driven by iid noise $Z_t$ with variance $\sigma^2=\frac{\nu}{\nu-2}=\frac{10}{8}=\frac{5}{4},$ we have asymptotic normality of $\hat\phi$ with corresponding asymptotic mean $\phi$ and asymptotic variance $\frac{\sigma^2\Gamma_p^{-1}}{n}$ i.e.
$$
\hat\phi_n\asympt\nd{\phi}{\frac{\sigma^2\Gamma_{p=1}^{-1}}{n}}
$$
or equivalently
$$
\sqrt{n}\lrp{\hat\phi-\phi}\rad\nd{0}{\sigma^2\Gamma_{1}^{-1}}.
$$
<!-- Asymptotic confidence intervals!!!! - see possibly Vidsand1 20-21 p. 259 YES!!! -->
<!-- P3.3 and existence of Gamma^-1 - see p 41. for linear processes (in particular causal ARMA(p,q) processes) being strictly stationary and Ergodic. -->
With this we may for our fixed $n=1000$ determine that $\lrp{\phi - \frac{1.96}{\sqrt{n}}\sqrt{\sigma^2\Gamma_1^{-1}},\,\phi + \frac{1.96}{\sqrt{n}}\sqrt{\sigma^2\Gamma_1^{-1}}}$ will be an asymptotic $95\%$ confidence interval for $\phi.$
Estimating $\Gamma_1$ via the sample autocovariance function, we find:
$$
\tilde\Gamma_1:=\gamma_{n,X}\lrp{1-1}=\gamma_{n,X}\lrp{0}=`r gamma(X,0)`
$$
such that 
$$
\tilde\Gamma_1^{-1}=\frac{1}{\tilde\Gamma_1}=`r 1/gamma(X,0)`\neq0
$$
<!-- Should this be the proper $gamma_X$ or the sample autocovariance function? -->
<!-- Noting that $\Gamma_1=\gamma_X -->
such that we may rewrite the confidence bands as
$$
\lrp{\phi - \frac{1.96}{\sqrt{n}}\sqrt{\sigma^2`r 1/gamma(X,0)`},\,\phi + \frac{1.96}{\sqrt{n}}\sqrt{\sigma^2`r 1/gamma(X,0)`}}
$$
<!-- Should we calculate with estimates for $\phi$ and $\sigma^2$? -->
Inserting the other estimates and $n=1000$ we get
\begin{equation*}
\lrp{\hat\phi - \frac{1.96}{\sqrt{10^3}}\sqrt{\hat\sigma^2`r 1/gamma(X,0)`},\,\hat\phi + \frac{1.96}{\sqrt{10^3}}\sqrt{\hat\sigma^2`r 1/gamma(X,0)`}}\\
=\lrp{`r phih` - \frac{1.96}{\sqrt{10^3}}\sqrt{`r sigmah2*1/gamma(X,0)`},\,`r phih` + \frac{1.96}{\sqrt{10^3}}\sqrt{`r sigmah2*1/gamma(X,0)`}}\\
=\lrp{`r phih - 1.96/(sqrt(n))*sqrt(sigmah2*1/gamma(X,0))`,`r phih + 1.96/(sqrt(n))*sqrt(sigmah2*1/gamma(X,0))`}
\end{equation*}

### c)

#### i+ii)

We calculate the residuals:
```{r}
Zh <- rep(NA,n)
Zh[1] <- X[1]
for (j in 2:n) {
   Zh[j] <- X[j]+phih*X[j-1]
}
```

We do a reordering of these in the requested fashion using `sample`:
```{r}
Zhs<-sample(Zh)
```

We define a new sample:
```{r}
Xhs <- rep(NA,n)
Xhs[1] <- Zhs[1]
for (j in 2:n) {
   Xhs[j] <- phih*Xhs[j-1]+Zhs[j]
}
```

#### iii)

##### part1)
As in b)
```{r}
(hsrho1 <- gamma(Xhs,1)/gamma(Xhs,0))
(hsphih <- hsrho1)
```


##### part2)
We repeat the tasks done in the previous exercises by writing a function to do so

```{r, fig.align='center'}
boots <- function(Zh,n) { #Simulating the Bootstrap data
   q <- length(Zh)
   XhsM <- matrix(NA,nrow=n,ncol=q)
   for (i in 1:n) {
      Zhsi <- sample(Zh)
      XhsM[i,1] <- Zhsi[1]
      for (j in 2:q) {
        XhsM[i,j] <- phih*XhsM[i,j-1]+Zhsi[j]
      }
   }
   XhsM
}

dat <- boots(Zh,1000)
YW <- function(Zh,n) { #Calculating the YW's
   q <- length(Zh)
   dat <- boots(Zh,n)
   phihV <- rep(NA,n)
   for (i in 1:n) {
      phihV[i] <- gamma(dat[i,],1)/gamma(dat[i,],0)
   }
   phihV
}
phihV <- YW(Zh,n)
hist(phihV)
boxplot(phihV)
quantile(phihV, c(0.025, 0.975))
```

Comparing this confidence interval to the asymptotic one achieved in b):
```{r}
rbind(quantile(phihV, c(0.025, 0.975)), c(phih - 1.96/(sqrt(n))*sqrt(sigmah2*1/gamma(X,0)),phih + 1.96/(sqrt(n))*sqrt(sigmah2*1/gamma(X,0))))
```
we notice a great similarity, though the asymptotic confidence interval seems shifted approximately $\cong0.007$ in comparison to the bootstrap interval. 



## Exercise 2
We may import the data
```{r}
Data <- read_csv("Data.csv",col_types =
                       cols(col_date(),
                            col_double(),
                            col_double(),
                            col_double(),
                            col_double(),
                            col_double(),
                            col_integer()))
head(Data, 10)
```
We may create a yearly data set, and filter for data after 2000 and remove `NA`'s
```{r}
Data_new <- Data %>% filter(year(Date)>=2000) %>%  mutate(returns = (Close - lag(Close))/lag(Close), 
                                                  logreturns = log(1+returns),
                                                  abslogreturn = abs(logreturns),
                                                  absdiff = abs(returns - logreturns))
Data_new_clean <- Data_new %>% na.omit()
Data_new_new <- Data_new %>% group_by(year(Date)) %>% rename('year' = 'year(Date)') %>% summarise(mean = mean(logreturns, na.rm = T), var = var(logreturns, na.rm = T), absmean = mean(abs(logreturns), na.rm = T), absvar = var(abs(logreturns), na.rm = T))
```

We plot these:
<!-- Fix legend -->
```{r}
ggplot(Data_new_new) + geom_point(aes(x=year, y=mean), colour = 'blue') + geom_point(aes(x=year, y=var), colour = 'red') + geom_point(aes(x=year, y=absmean), colour = 'hotpink') + geom_point(aes(x=year, y=absvar), colour = 'orange')
```

None of the requested quantiles vary a lot, so we cannot reject the possibility of underlying ergodicity.

## Exercise 3

### a)
We might plot the absolute differences:
```{r, fig.align='center'}
ggplot(Data_new_clean, aes(x=Date, y=absdiff)) + geom_point(alpha = 0.2) + ggtitle("Absolute differences over time")
```
and calculate the maximum of the absolute difference between returns and logreturns:
```{r}
max(Data_new_clean$absdiff)
```

### b)

We note that the dataset `Data_new_clean` has `nrow(Data_new_clean) =`r nrow(Data_new_clean)`` data points, such that 10% of the sample size of the log return time series will be of the size `floor(nrow(Data_new_clean)/10) =`r floor(nrow(Data_new_clean)/10)``. We may use `acf` to calculate this many lags for the sample autocorrelation function for the log-return time series, its absolute value, and its square:
```{r}
autocorLogRet <- acf(Data_new_clean$logreturns, lag.max = floor(nrow(Data_new_clean)/10), plot=F)
autocorLogRetAbs <- acf(Data_new_clean$abslogreturn, lag.max = floor(nrow(Data_new_clean)/10), plot=F)
autocorLogRetSquared <- acf((Data_new_clean$logreturns)^2, lag.max = floor(nrow(Data_new_clean)/10), plot=F)
```

We may also choose to plot each of these:
```{r, fig.align='center'}
acf(Data_new_clean$logreturns, lag.max = floor(nrow(Data_new_clean)/10), plot=T)
acf(Data_new_clean$abslogreturn, lag.max = floor(nrow(Data_new_clean)/10), plot=T)
acf((Data_new_clean$logreturns)^2, lag.max = floor(nrow(Data_new_clean)/10), plot=T)
```

### c)
We will fit the AR model using `ar.yw`
```{r}
Data_logreturn <- Data_new_clean[,9][[1]]
head(Data_logreturn)
ts.plot(Data_logreturn)
modellr <- ar.yw(Data_logreturn, aic = T)
```

We may see a summary of the model:
```{r}
print(modellr)
summary(modellr)
```

### d)

We may simulate based on the built up model:
```{r}
modellr$ar
model3d <- arima.sim(model = list(ar = modellr$ar), n = length(Data_logreturn), rand.gen = function(n,...) rt(n,df=4))
summary(model3d)
```
We may plot the series, together with the original log-returns:
```{r, fig.align='center'}
plot(model3d)
ts.plot(Data_logreturn)
```

and the requested autocorrelations:
```{r, fig.align='center'}
acf(model3d,lag.max = floor(nrow(Data_new_clean)/10), plot=T)
```

```{r, fig.align='center'}
acf(abs(model3d), lag.max = floor(nrow(Data_new_clean)/10), plot=T)
```

````{r, fig.align='center'}
acf(model3d^2, lag.max = floor(nrow(Data_new_clean)/10), plot=T)
```

We might note that comparing the plot of `model3d` to the log-return series, a striking difference appears, in that the model seems to attain values orders of magnitude higher, than there is in the original log-returns time series:

We may also note that while the autocorrelation function of the untransformed `model3d` looks rather similar to that of untransformed log-return series, the transformations appear dissimilar, possibly hinting at another problem. 
It seems that `model3d` as a model does not perform adequately in these circumstances, such that a different model, or a different choice of noise term might be needed.


## Exercise 4

### a)

Noting that $\rho_X\lrp{h}=\phi^{\lrb{h}}$ we may compute
\begin{align}
w_{hh}&=\sum_{k=1}^{\infty}{\lrp{\rho_X\lrp{k+h}+\rho_X\lrp{k-h}-2\rho_X\lrp{h}\rho_X\lrp{k}}^2}\\
&=\sum_{k=1}^{\infty}{\lrp{\phi^{\lrb{k+h}}+\phi^{\lrb{k-h}}-\phi^{\lrb{k}}\phi^{\lrb{h}}}^2}\\
&=\sum_{k=1}^{\infty}{\lrp{\phi^{\lrb{k-h}}-\phi^{\lrb{k+h}}}^2}\\
&=\sum_{k=1}^{h}{\lrp{\phi^{h-k}-\phi^{k+h}}^2}+\sum_{k=h+1}^{\infty}{\lrp{\phi^{k-h}-\phi^{k+h}}^2}\\
&=\sum_{k=1}^{h}{\phi^{2h}\lrp{\phi^{-k}-\phi^{k}}^2}+\sum_{k=h+1}^{\infty}{\phi^{2k}\lrp{\phi^{-h}-\phi^{h}}^2}\\
&=\phi^{2h}\sum_{k=1}^{h}{\lrp{\phi^{-k}-\phi^{k}}^2}+\lrp{\phi^{-h}-\phi^{h}}^2\sum_{k=h+1}^{\infty}{\phi^{2k}}\\
&=-\frac{\phi^{2h}\lrp{1+2h - \phi^2-2h \phi^2 - \phi^{-2h}+\phi^{2+2h}}}{1-\phi^2}+\frac{\phi^{2h+2}\lrp{\phi^{-h}-\phi^h}^2}{1-\phi^2}
\end{align}
<!-- # -->
such that for $0<\phi<1$ we may conclude that for $h\ra\infty$
\begin{align}
w_{hh}\ra \frac{1}{1-\phi^2}+\frac{\phi^2}{1-\phi^2}.
\end{align}

### b)

We will simulate a size $200$ sample from the AR(1) process as follows:
```{r}
sim4b <- arima.sim(model = list(ar = 0.8), n=200)
ts.plot(sim4b)
```

We may draw the sample auotcorrelations, along with the confidence band for the iid white noise for a $25$ lag:
```{r}
acf(sim4b, lag.max = 25)
```

for each lag up to $25$ we may do the calculation of the $95\%$ asymptotic confidence bands from a)
```{r}
n4b <- 200
phi4b <- 0.8
rhonX4b <- as.numeric(acf(sim4b, lag.max = 25, plot=F)[[1]])
w <- 1:25
autocorconfp <- 1:25
autocorconfm <- 1:25
for (h in w) {
   w[h] <- -((phi4b^(2*h))*(1+2*h-phi4b^2-2*h*phi4b^2-phi4b^(-2*h)+phi4b^(2+2*h)))/(1-phi4b^2)+(phi4b^(2*h+2)*(phi4b^(-h)-phi4b^h)^2)/(1-phi4b^2)
   autocorconfp[h] <- rhonX4b[h]+qnorm(0.975)*sqrt(w[h]/n4b)
   autocorconfm[h] <- rhonX4b[h]-qnorm(0.975)*sqrt(w[h]/n4b)
}
autocorconf <- rbind(autocorconfp, autocorconfm)
```

We may plot the asymptotic correlation confidence bands together with the previous sample auto correlation function
```{r}
acf(sim4b, lag.max = 25, ylim = c(-1,1));lines(1:25, autocorconfp, lty = 3);lines(1:25, autocorconfm, lty = 3)
```


## Exercise 5
We may import the dataset, convert it to `xts` and calculate last day of each year
```{r}
ss <- as.xts(sunspots)
ep <- ss %>% endpoints("years")
```

With this, we will aggregate the data averaging over each year:
```{r}
ssy <- ss %>% period.apply(INDEX = ep, FUN = mean)
head(ssy)
```


### a)
We may calculate AIC using the `ar.yw` command:
```{r, fig.align='center'}
model5a <- ar.yw(ssy, order.max= 20)
model5a$aic
plot(model5a$aic)
```

### b)
We may let `ar.yw` choose the order of the AR model that minimizes the AIC over `ssy`:
```{r}
model5b <- ar.yw(ssy, aic = T)
print(model5b)
summary(model5b)
```
We may then simulate from the model
```{r}
simModel5b1 <- arima.sim(model = list(ar = model5b$ar), n = length(ssy))
```

And provide the respective plots:
```{r, fig.align='center'}
ts.plot(simModel5b1)
ts.plot(ssy)
```

It seems that the noise terms in the `simModel5b1` that by default are chosen by `arima.sim` to be iid standard normally distributed have one major problem: `simModel5b1` attains the wrong range of values, than required in order to model `ssy`

We will attempt to solve these problems by implementing the suggested solutions to these problems provided.
Staying within iid normal noise, we may take a look at the residuals of `model5b` attain their mean and standard deviation, in order to attempt a non-standard normal noise term model
```{r}
head(model5b$resid,15)
mean(model5b$resid, na.rm = T)
sd(model5b$resid, na.rm = T)
hist(model5b$resid, prob = T, breaks = 20);lines(seq(-40,40,by=0.01), dnorm(seq(-40,40,by=0.01), mean = mean(model5b$resid, na.rm = T), sd = sd(model5b$resid, na.rm = T)))
```

We note from the simple histogram plot above that the fit is adequate, but with resolution lost in the center, probably caused by a few larger values dragging out the summary statistics. 

```{r}
simModel5b2 <- arima.sim(model = list(ar = model5b$ar), n = length(ssy), rand.gen = function(n,...) rnorm(n,mean(model5b$resid, na.rm = T),sd(model5b$resid, na.rm = T)))
```

And provide the respective plots:
```{r}
ts.plot(simModel5b2)
ts.plot(ssy)
```
we see that the variability of `simModel5b2` is now appropriate, though still attaining negative values thus requiring further investigation.

```{r}
simModel5b2new <- arima.sim(model = list(ar = model5b$ar), n = length(ssy), rand.gen = function(n,...) rcauchy(n,mean(model5b$resid, na.rm = T),sd(model5b$resid, na.rm = T)))
```


Permuting the residuals, we get the following model
```{r}
sRes <- sample(model5b$resid)
simModel5b3 <- arima.sim(model = list(ar = model5b$coefficients), n = length(ssy), rand.gen = function(...) sRes)
ts.plot(simModel5b3)
```
this however seems to attain too large a degree of variability, than what is 'required' for the data. 


### c)

We note that the simulated model attains the same kind of occilatory autocorrelation behaviour as `ssy`, if not slightly less provounced. 
```{r}
acf(ssy)
acf(simModel5b2)
```



